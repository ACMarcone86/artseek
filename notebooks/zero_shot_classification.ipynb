{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicola/miniconda3/envs/vitart/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from safetensors.torch import save_file, load_file\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import lovely_tensors as lt\n",
    "from torchmetrics.classification import Accuracy\n",
    "\n",
    "lt.monkey_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"MATCH (a:Artist) -- (artwork:Artwork) WITH a, COUNT(artwork) AS num_artworks WHERE num_artworks > 50 RETURN a.name\"\n",
    "result = graph.query(query)\n",
    "artists = [record['a.name'] for record in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties are the following:\n",
      "Genre {name: STRING},Style {name: STRING, summary: STRING, wikipedia_url: STRING},Artist {dbpedia_url: STRING, printed_name: STRING, image_url: STRING, birth_date: STRING, wikipedia_url: STRING, name: STRING, biography: STRING, gender: STRING, death_date: STRING, death_place: STRING, birth_place: STRING},Media {name: STRING},Tag {name: STRING},Artwork {date: STRING, title: STRING, name: STRING, image_url: STRING, dimensions: STRING, wikidata_url: STRING, described_at_url: STRING, wikipedia_url: STRING},Movement {name: STRING},Training {name: STRING},Subject {name: STRING},Field {name: STRING},People {name: STRING},Serie {name: STRING},Period {name: STRING},Gallery {name: STRING},City {name: STRING},Country {name: STRING},Emotion {name: STRING}\n",
      "Relationship properties are the following:\n",
      "elicits {description: STRING, arousal: INTEGER}\n",
      "The relationships are the following:\n",
      "(:Artist)-[:belongsToMovement]->(:Movement),(:Artist)-[:hasSubject]->(:Subject),(:Artist)-[:belongsToField]->(:Field),(:Artist)-[:relatedToSchool]->(:Training),(:Artist)-[:trainedBy]->(:Artist),(:Artist)-[:hasPatron]->(:People),(:Artwork)-[:madeOf]->(:Media),(:Artwork)-[:about]->(:Tag),(:Artwork)-[:hasGenre]->(:Genre),(:Artwork)-[:hasStyle]->(:Style),(:Artwork)-[:locatedIn]->(:Country),(:Artwork)-[:locatedIn]->(:City),(:Artwork)-[:locatedIn]->(:Gallery),(:Artwork)-[:createdBy]->(:Artist),(:Artwork)-[:elicits]->(:Emotion),(:Artwork)-[:hasPeriod]->(:Period),(:Artwork)-[:partOf]->(:Serie),(:Gallery)-[:inCountry]->(:Country),(:Gallery)-[:inCity]->(:City),(:City)-[:inCountry]->(:Country)\n"
     ]
    }
   ],
   "source": [
    "print(graph.get_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "artworks = {}\n",
    "for artist in artists:\n",
    "    query = f\"MATCH (a:Artist {{name: '{artist}'}}) -- (artwork:Artwork) RETURN artwork.name\"\n",
    "    result = graph.query(query)\n",
    "    artworks[artist] = [record['artwork.name'] for record in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "artists = []\n",
    "for artist in artworks:\n",
    "    for artwork in artworks[artist]:\n",
    "        images.append(artwork)\n",
    "        artists.append(artist)\n",
    "\n",
    "df = pd.DataFrame({\"image\": images, \"artist\": artists})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"artist\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"MATCH (artwork:Artwork) RETURN artwork.name\"\n",
    "result = graph.query(query)\n",
    "artwork_names = [record['artwork.name'] for record in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = Path(\"data/images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"clip_features.safetensors\"):\n",
    "    tensors = {}\n",
    "    batch_size = 128\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(artwork_names), batch_size)):\n",
    "            batch = artwork_names[i:i+batch_size]\n",
    "            imgs = [Image.open(IMG_DIR / img) for img in batch]\n",
    "            inputs = processor(images=imgs, return_tensors=\"pt\")\n",
    "            outputs = model.get_image_features(**inputs)\n",
    "            for j, img in enumerate(batch):\n",
    "                tensors[img] = outputs[j]\n",
    "else:\n",
    "    tensors = load_file(\"clip_features.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = sorted(list(set(df[\"artist\"])))\n",
    "artist_to_idx = {artist: idx for idx, artist in enumerate(artists)}\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=len(artists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [f\"An image of a painting by {artist.replace('-', ' ').title()}\" for artist in artists]\n",
    "inputs = processor(text=prompts, return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    text_features = model.get_text_features(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alfred-wallis'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"image\"] == \"alfred-wallis_ship-people-and-animals.jpg\"][\"artist\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16236it [00:49, 329.10it/s]\n"
     ]
    }
   ],
   "source": [
    "for row in tqdm(test_df.iterrows()):\n",
    "    key = row[1][\"image\"]\n",
    "    tensor = tensors[key]\n",
    "    similarities = torch.nn.functional.cosine_similarity(tensor.unsqueeze(0), text_features, dim=1)\n",
    "    pred = torch.argmax(similarities).item()\n",
    "    artist = df[df[\"image\"] == key][\"artist\"].values[0]\n",
    "    target = artist_to_idx[artist]\n",
    "    if pred == target:\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31522542498152256"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct / len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_centroids = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for artist in tqdm(artists):\n",
    "    artist_df = train_df[train_df[\"artist\"] == artist]\n",
    "    images = artist_df[\"image\"].values\n",
    "    artist_tensors = torch.stack([tensors[img] for img in images])\n",
    "    artist_tensor = artist_tensors.mean(dim=0)\n",
    "    artist_centroids[artist] = artist_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in tqdm(test_df.iterrows()):\n",
    "    key = row[1][\"image\"]\n",
    "    tensor = tensors[key]\n",
    "    similarities = torch.stack([torch.nn.functional.cosine_similarity(tensor.unsqueeze(0), centroid.unsqueeze(0), dim=1) for centroid in artist_centroids.values()])\n",
    "    pred = torch.argmax(similarities).item()\n",
    "    artist = df[df[\"image\"] == key][\"artist\"].values[0]\n",
    "    target = artist_to_idx[artist]\n",
    "    if pred == target:\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct / len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensors = torch.stack([tensors[img] for img in train_df[\"image\"].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[64944, 512] n=33251328 (0.1Gb) x∈[-10.952, 3.523] μ=0.004 σ=0.441"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16236it [19:02, 14.21it/s]\n"
     ]
    }
   ],
   "source": [
    "for row in tqdm(test_df.iterrows()):\n",
    "    key = row[1][\"image\"]\n",
    "    tensor = tensors[key]\n",
    "    similarities = torch.nn.functional.cosine_similarity(tensor.unsqueeze(0), train_tensors, dim=1)\n",
    "    pred = torch.argmax(similarities).item()\n",
    "    artist = df[df[\"image\"] == key][\"artist\"].values[0]\n",
    "    target = artist_to_idx[artist]\n",
    "    artist_pred = train_df.iloc[pred][\"artist\"]\n",
    "    pred = artist_to_idx[artist_pred]\n",
    "    if pred == target:\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47610248829761026"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct / len(test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vitart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
